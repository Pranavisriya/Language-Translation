# -*- coding: utf-8 -*-
"""PR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SVbbuyJfJxP2BbEe10jmQ1oeu27yUdqp
"""

import warnings
warnings.filterwarnings("ignore")
from transformers import AutoTokenizer
from transformers import AutoModelForSeq2SeqLM
import pandas as pd
import torch

def translate(input_sentence):
    tokenizer_bg = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-bg")
    model_bg = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-en-bg")
    model_bg.load_state_dict(torch.load("/Users/pranavisriya/Downloads/language_translation_PR/model_weights_en-bg.pth", map_location=torch.device('cpu')))
    model_bg.eval()
    inputs_bg = tokenizer_bg(input_sentence, max_length=128, padding=True, truncation=True, return_tensors="pt")
    translated_tokens_bg = model_bg.generate(**inputs_bg, max_length=128, num_beams=4, early_stopping=True)
    translated_sentence_bg = tokenizer_bg.decode(translated_tokens_bg[0], skip_special_tokens=True)

    tokenizer_fr = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-fr")
    model_fr = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-en-fr")
    model_fr.load_state_dict(torch.load("/Users/pranavisriya/Downloads/language_translation_PR/model_weights_en-fr.pth", map_location=torch.device('cpu')))
    model_fr.eval()
    inputs_fr = tokenizer_fr(input_sentence, max_length=128, padding=True, truncation=True, return_tensors="pt")
    translated_tokens_fr = model_fr.generate(**inputs_fr, max_length=128, num_beams=4, early_stopping=True)
    translated_sentence_fr = tokenizer_fr.decode(translated_tokens_fr[0], skip_special_tokens=True)

    tokenizer_sv = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-sv")
    model_sv = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-en-sv")
    model_sv.load_state_dict(torch.load("/Users/pranavisriya/Downloads/language_translation_PR/model_weights_en-sv.pth", map_location=torch.device('cpu')))
    model_sv.eval()
    inputs_sv = tokenizer_sv(input_sentence, max_length=128, padding=True, truncation=True, return_tensors="pt")
    translated_tokens_sv = model_sv.generate(**inputs_sv, max_length=128, num_beams=4, early_stopping=True)
    translated_sentence_sv = tokenizer_sv.decode(translated_tokens_sv[0], skip_special_tokens=True)
    if len(input_sentence)<2:
        translated_sentence_bg="Please enter valid input"
        translated_sentence_fr="Please enter valid input"
        translated_sentence_sv="Please enter valid input"
    data = {
    'language': ['english', 'bg', 'fr', 'sv'],
    'translated_text': [
        input_sentence,
        translated_sentence_bg,
        translated_sentence_fr,
        translated_sentence_sv
    ]
    }
    df = pd.DataFrame(data)
    return df.to_html(classes='translated-table')